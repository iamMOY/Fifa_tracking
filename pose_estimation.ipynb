{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "!nvcc --version\n",
    "torch_version = '.'.join(torch.__version__.split('.')[:2])\n",
    "\n",
    "print(\"Torch version: {}\".format(torch_version,cuda_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "!git clone https://github.com/WongKinYiu/yolov7\n",
    "%cd {HOME}/yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f\"{HOME}/yolov7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "!mkdir input\n",
    "%cd {HOME}/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOURCE_VIDEO_A_PATH = f\"{HOME}/input/file-1.mp4\" Video file from one angle\n",
    "#SOURCE_VIDEO_B_PATH = f\"{HOME}/input/file-2.mp4\" Same video from different angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def generate_frame(video_file: str) -> Generator[np.ndarray,None,None]:\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    while video.isOpened():\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        yield frame\n",
    "    video.release()\n",
    "\n",
    "\n",
    "def plot_image(image: np.ndarray, size: int = 12) -> None:\n",
    "    plt.figure(figsize=(size,size))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image[...,::-1])\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "    @property\n",
    "    def int_xy_tuple(self) -> Tuple[int, int]:\n",
    "        return int(self.x), int(self.y)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Rect:\n",
    "    x: float\n",
    "    y: float\n",
    "    width: float\n",
    "    height: float\n",
    "\n",
    "    @property\n",
    "    def top_left(self) -> Point:\n",
    "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
    "        \n",
    "    @property\n",
    "    def bottom_right(self) -> Point:\n",
    "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
    "    \n",
    "    @property\n",
    "    def bottom_center(self) -> Point:\n",
    "        return Point(x=self.x + self.width / 2, y=self.y + self.height)\n",
    "        \n",
    "@dataclass\n",
    "class Detection:\n",
    "    rect: Rect\n",
    "    class_id: int\n",
    "    confidence: float\n",
    "    tracker_id: Optional[int] = None\n",
    "        \n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class Color:\n",
    "    r: int\n",
    "    g: int\n",
    "    b: int\n",
    "        \n",
    "    @property\n",
    "    def bgr_tuple(self) -> Tuple[int, int, int]:\n",
    "        return self.b, self.g, self.r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def draw_rect(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
    "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, thickness)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
    "\n",
    "frame = next(frame_iterator)\n",
    "plot_image(frame, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}/yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt --quiet\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_MODEL_WEIGHTS_PATH = f\"{HOME}/yolov7/yolov7-e6e.pt\"\n",
    "POSE_MODEL_WEIGHTS_PATH = f\"{HOME}/yolov7/yolov7-w6-pose.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.general import check_img_size\n",
    "from models.experimental import attempt_load\n",
    "\n",
    "detection_model = attempt_load(weights=DETECTION_MODEL_WEIGHTS_PATH, map_location=device)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigths = torch.load(POSE_MODEL_WEIGHTS_PATH, map_location=device)\n",
    "pose_model = weigths[\"model\"]\n",
    "_ = pose_model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pose_model.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_IMAGE_SIZE = 1920\n",
    "POSE_IMAGE_SIZE = 960\n",
    "STRIDE = 64\n",
    "CONFIDENCE_TRESHOLD = 0.25\n",
    "IOU_TRESHOLD = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import letterbox\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def detection_pre_process_frame(frame: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    img = letterbox(frame, DETECTION_IMAGE_SIZE, STRIDE, auto=True)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device).float()\n",
    "    img /= 255.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def pose_pre_process_frame(frame: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    image = letterbox(frame, POSE_IMAGE_SIZE, stride=STRIDE, auto=True)[0]\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from utils.general import non_max_suppression_kpt, non_max_suppression\n",
    "from utils.plots import output_to_keypoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def clip_coords(boxes: np.ndarray, img_shape: Tuple[int, int]):\n",
    "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    boxes[:, 0] = np.clip(boxes[:, 0], 0, img_shape[1]) # x1\n",
    "    boxes[:, 1] = np.clip(boxes[:, 1], 0, img_shape[0]) # y1\n",
    "    boxes[:, 2] = np.clip(boxes[:, 2], 0, img_shape[1]) # x2\n",
    "    boxes[:, 3] = np.clip(boxes[:, 3], 0, img_shape[0]) # y2\n",
    "\n",
    "\n",
    "def detection_post_process_output(\n",
    "    output: torch.tensor, \n",
    "    confidence_trashold: float, \n",
    "    iou_trashold: float,\n",
    "    image_size: Tuple[int, int],\n",
    "    scaled_image_size: Tuple[int, int]\n",
    ") -> np.ndarray:\n",
    "    output = non_max_suppression(\n",
    "        prediction=output,\n",
    "        conf_thres=confidence_trashold,\n",
    "        iou_thres=iou_trashold\n",
    "    )\n",
    "    coords = output[0].detach().cpu().numpy()\n",
    "    \n",
    "    v_gain = scaled_image_size[0] / image_size[0]\n",
    "    h_gain = scaled_image_size[1] / image_size[1]\n",
    "\n",
    "    coords[:, 0] /= h_gain\n",
    "    coords[:, 1] /= v_gain\n",
    "    coords[:, 2] /= h_gain\n",
    "    coords[:, 3] /= v_gain\n",
    "\n",
    "    clip_coords(coords, image_size)\n",
    "    return coords\n",
    "\n",
    "\n",
    "def post_process_pose(pose: np.ndarray, image_size: Tuple, scaled_image_size: Tuple) -> np.ndarray:\n",
    "    height, width = image_size\n",
    "    scaled_height, scaled_width = scaled_image_size\n",
    "    vertical_factor = height / scaled_height\n",
    "    horizontal_factor = width / scaled_width\n",
    "    result = pose.copy()\n",
    "    for i in range(17):\n",
    "        result[i * 3] = horizontal_factor * result[i * 3]\n",
    "        result[i * 3 + 1] = vertical_factor * result[i * 3 + 1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def pose_post_process_output(\n",
    "    output: torch.tensor, \n",
    "    confidence_trashold: float, \n",
    "    iou_trashold: float,\n",
    "    image_size: Tuple[int, int],\n",
    "    scaled_image_size: Tuple[int, int]\n",
    ") -> np.ndarray:\n",
    "    output = non_max_suppression_kpt(\n",
    "        prediction=output, \n",
    "        conf_thres=confidence_trashold, \n",
    "        iou_thres=iou_trashold, \n",
    "        nc=pose_model.yaml['nc'], \n",
    "        nkpt=pose_model.yaml['nkpt'], \n",
    "        kpt_label=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "\n",
    "        for idx in range(output.shape[0]):\n",
    "            output[idx, 7:] = post_process_pose(\n",
    "                output[idx, 7:], \n",
    "                image_size=image_size,\n",
    "                scaled_image_size=scaled_image_size\n",
    "            )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import plot_skeleton_kpts\n",
    "\n",
    "\n",
    "def detect_annotate(image: np.ndarray, detections: np.ndarray, color: Color, thickness: int = 2) -> np.ndarray:\n",
    "    annotated_image = image.copy()\n",
    "    for x_min, y_min, x_max, y_max, confidence, class_id in detections:\n",
    "        rect = Rect(\n",
    "            x=float(x_min),\n",
    "            y=float(y_min),\n",
    "            width=float(x_max - x_min),\n",
    "            height=float(y_max - y_min)\n",
    "        )\n",
    "        annotated_image = draw_rect(image=annotated_image, rect=rect, color=color, thickness=thickness)\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "\n",
    "def pose_annotate(image: np.ndarray, detections: np.ndarray) -> np.ndarray:\n",
    "    annotated_frame = image.copy()\n",
    "\n",
    "    for idx in range(detections.shape[0]):\n",
    "        pose = detections[idx, 7:].T\n",
    "        plot_skeleton_kpts(annotated_frame, pose, 3)\n",
    "\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COLOR = Color(r=255, g=255, b=255)\n",
    "\n",
    "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
    "\n",
    "frame = next(frame_iterator)\n",
    "\n",
    "detection_pre_processed_frame = detection_pre_process_frame(\n",
    "    frame=frame, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "image_size = frame.shape[:2]\n",
    "scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
    "\n",
    "    detection_output = detection_post_process_output(\n",
    "        output=detection_output,\n",
    "        confidence_trashold=CONFIDENCE_TRESHOLD,\n",
    "        iou_trashold=IOU_TRESHOLD,\n",
    "        image_size=image_size,\n",
    "        scaled_image_size=scaled_image_size\n",
    "    )\n",
    "\n",
    "annotated_frame = detect_annotate(image=frame, detections=detection_output, color=COLOR)\n",
    "\n",
    "plot_image(annotated_frame, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_and_annotate(frame: np.ndarray) -> np.ndarray:\n",
    "    pose_pre_processed_frame = pose_pre_process_frame(frame=frame.copy(), device=device)\n",
    "    \n",
    "    image_size = frame.shape[:2]\n",
    "    scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pose_output, _ = pose_model(pose_pre_processed_frame)\n",
    "        pose_output = pose_post_process_output(\n",
    "            output=pose_output,\n",
    "            confidence_trashold=CONFIDENCE_TRESHOLD, \n",
    "            iou_trashold=IOU_TRESHOLD,\n",
    "            image_size=image_size,\n",
    "            scaled_image_size=scaled_image_size\n",
    "        )\n",
    "\n",
    "    annotated_frame = pose_annotate(image=frame, detections=pose_output)\n",
    "\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
    "\n",
    "frame = next(frame_iterator)\n",
    "\n",
    "annotated_frame = process_frame_and_annotate(frame=frame)\n",
    "\n",
    "plot_image(annotated_frame, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_B_PATH))\n",
    "\n",
    "frame = next(frame_iterator)\n",
    "\n",
    "annotated_frame = process_frame_and_annotate(frame=frame)\n",
    "\n",
    "plot_image(annotated_frame, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "usage example:\n",
    "\n",
    "video_config = VideoConfig(\n",
    "    fps=30, \n",
    "    width=1920, \n",
    "    height=1080)\n",
    "video_writer = get_video_writer(\n",
    "    target_video_path=TARGET_VIDEO_PATH, \n",
    "    video_config=video_config)\n",
    "\n",
    "for frame in frames:\n",
    "    ...\n",
    "    video_writer.write(frame)\n",
    "    \n",
    "video_writer.release()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# stores information about output video file, width and height of the frame must be equal to input video\n",
    "@dataclass(frozen=True)\n",
    "class VideoConfig:\n",
    "    fps: float\n",
    "    width: int\n",
    "    height: int\n",
    "        \n",
    "\n",
    "# create cv2.VideoWriter object that we can use to save output video\n",
    "def get_video_writer(target_video_path: str, video_config: VideoConfig) -> cv2.VideoWriter:\n",
    "    video_target_dir = os.path.dirname(os.path.abspath(target_video_path))\n",
    "    os.makedirs(video_target_dir, exist_ok=True)\n",
    "    return cv2.VideoWriter(\n",
    "        target_video_path, \n",
    "        fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"), \n",
    "        fps=video_config.fps, \n",
    "        frameSize=(video_config.width, video_config.height), \n",
    "        isColor=True\n",
    "    )\n",
    "\n",
    "\n",
    "def get_frame_count(path: str) -> int:\n",
    "    cap = cv2.VideoCapture(SOURCE_VIDEO_A_PATH)\n",
    "    return int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH = SOURCE_VIDEO_B_PATH\n",
    "TARGET_VIDEO_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-b.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "COLOR = Color(r=255, g=255, b=255)\n",
    "\n",
    "# initiate video writer\n",
    "video_config = VideoConfig(\n",
    "    fps=25, \n",
    "    width=1920, \n",
    "    height=1080)\n",
    "video_writer = get_video_writer(\n",
    "    target_video_path=TARGET_VIDEO_PATH, \n",
    "    video_config=video_config)\n",
    "\n",
    "# get fresh video frame generator\n",
    "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))\n",
    "\n",
    "total = get_frame_count(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for frame in tqdm(frame_iterator, total=total):\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_size = frame.shape[:2]\n",
    "\n",
    "        #detection\n",
    "        detection_pre_processed_frame = detection_pre_process_frame(\n",
    "            frame=frame, \n",
    "            device=device)\n",
    "        detection_scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
    "\n",
    "        detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
    "        detection_output = detection_post_process_output(\n",
    "            output=detection_output,\n",
    "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
    "            iou_trashold=IOU_TRESHOLD,\n",
    "            image_size=image_size,\n",
    "            scaled_image_size=detection_scaled_image_size\n",
    "        )\n",
    "        annotated_frame = detect_annotate(\n",
    "            image=annotated_frame, detections=detection_output, color=COLOR)\n",
    "\n",
    "        # pose\n",
    "        pose_pre_processed_frame = pose_pre_process_frame(\n",
    "            frame=frame, \n",
    "            device=device)\n",
    "        pose_scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
    "\n",
    "        pose_output = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
    "        pose_output = pose_post_process_output(\n",
    "            output=pose_output,\n",
    "            confidence_trashold=CONFIDENCE_TRESHOLD, \n",
    "            iou_trashold=IOU_TRESHOLD,\n",
    "            image_size=image_size,\n",
    "            scaled_image_size=pose_scaled_image_size\n",
    "        )\n",
    "        annotated_frame = pose_annotate(\n",
    "            image=annotated_frame, detections=pose_output)\n",
    "\n",
    "        # save video frame\n",
    "        video_writer.write(annotated_frame)\n",
    "\n",
    "# close output video\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def create_parent_dir(file_path: str) -> None:\n",
    "    file_directory = os.path.dirname(os.path.abspath(file_path))\n",
    "    os.makedirs(file_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def dump_json_file(file_path: str, content: Union[list, dict], **kwargs) -> None:\n",
    "    create_parent_dir(file_path=file_path)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(content, file, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_dump(source_video_path: str, target_json_path: str) -> None:\n",
    "    frame_iterator = iter(generate_frames(video_file=source_video_path))\n",
    "    total = get_frame_count(source_video_path)\n",
    "    entries = []\n",
    "\n",
    "    for frame in tqdm(frame_iterator, total=total):\n",
    "\n",
    "        image_size = frame.shape[:2]\n",
    "\n",
    "        #detection\n",
    "        detection_pre_processed_frame = detection_pre_process_frame(\n",
    "            frame=frame, \n",
    "            device=device)\n",
    "        detection_scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
    "\n",
    "        detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
    "        detection_output = detection_post_process_output(\n",
    "            output=detection_output,\n",
    "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
    "            iou_trashold=IOU_TRESHOLD,\n",
    "            image_size=image_size,\n",
    "            scaled_image_size=detection_scaled_image_size\n",
    "        )\n",
    "        \n",
    "        # pose\n",
    "        pose_pre_processed_frame = pose_pre_process_frame(\n",
    "            frame=frame, \n",
    "            device=device)\n",
    "        pose_scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
    "\n",
    "        pose_output = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
    "        pose_output = pose_post_process_output(\n",
    "            output=pose_output,\n",
    "            confidence_trashold=CONFIDENCE_TRESHOLD, \n",
    "            iou_trashold=IOU_TRESHOLD,\n",
    "            image_size=image_size,\n",
    "            scaled_image_size=pose_scaled_image_size\n",
    "        )\n",
    "\n",
    "        entry = {\n",
    "            \"detection\": detection_output.tolist(),\n",
    "            \"pose\": pose_output.tolist()\n",
    "        }\n",
    "        entries.append(entry)\n",
    "\n",
    "    dump_json_file(file_path=target_json_path, content=entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_dump(SOURCE_VIDEO_B_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-b.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47754cd3d5cc39ae07e03ddbfc49b120499a3234facacb077b7bd5eb1a4e0d9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
